[DEFAULT]
version = 2021.05.01.test
description = Config to test the tech grants pipeline. Uses small datasets to create training data, train models and evaluate. RF or EPMC data evaluated on was created by running `python nutrition_labels/create_training_data.py --config_path configs/training_data/2021.03.29.epmc.ini` and `python nutrition_labels/create_training_data.py --config_path configs/training_data/2021.03.29.rf.ini` - these create data/processed/training_data/210329epmc/training_data.csv and data/processed/training_data/210329rf/training_data.csv respectively.

[data]
# Variables used in create_training_data.py
epmc_tags_query_one_filedir = data/raw/expanded_tags/EPMC_relevant_tool_pubs_3082020.csv
epmc_tags_query_two_filedir = data/raw/expanded_tags/EPMC_relevant_pubs_query2_3082020.csv
epmc_pmid2grants_dir = data/raw/EPMC/pmid2grants.json
rf_tags_filedir = data/raw/expanded_tags/research_fish_manual_edit.csv
grant_tags_filedir = data/raw/expanded_tags/wellcome-grants-awarded-2005-2019_manual_edit_Lizadditions.csv
prodigy_filedir = data/prodigy/merged_tech_grants/merged_tech_grants.jsonl
grant_data_filedir = data/raw/wellcome-grants-awarded-2005-2019.csv
sources_include = ['Prodigy grants', 'Grants']
# Variables used in grant_tagger.py to train model
training_data_file = data/processed/training_data/210501test/training_data.csv
label_name = Relevance code
training_data_file_id = Internal ID
prediction_cols = ['Title', 'Description']
grants_text_data_file = data/raw/wellcome-grants-awarded-2005-2019.csv
grants_text_data_file_id = Internal ID

[prediction_data]
# Variables used in predict.py and evaluate.py
grants_data_path = data/raw/wellcome-grants-awarded-2005-2019_test_sample.csv
grant_text_cols = Title,Description
grant_id_col = Internal ID
# Variables used in evaluate.py
epmc_file_dir = data/processed/training_data/210329epmc/training_data.csv
rf_file_dir = data/processed/training_data/210329rf/training_data.csv
eval_id_col = Internal ID
eval_label_name = Relevance code

[data_col_ranking]
# Variables used in create_training_data.py - if multiple people annotate the data their annotations 
# will be in different columns, so these variables give the rank order of source of truth in the annotation columns.
# This is important for times where there is disagreement on the label.
# This can be set to a list with just one column name if there are not multiple annotators.
epmc_col_ranking = ['Revised code']
grants_col_ranking = ['Revised code']

[models]
# Variables used in grant_tagger.py to train model
vectorizer_types = ['count', 'tfidf']
classifier_types = ['naive_bayes', 'SVM']

[params]
# Variables used in grant_tagger.py to train model
relevant_sample_ratio = 1
test_size = 0.25
split_seed = 1

[model_parameters]
# Variables used in predict.py
num_agree = 1
pred_prob_thresh = 0.55
model_dirs = models/210501test/count_naive_bayes_210501test

[ensemble_model]
# Variables used in evaluate.py
num_agree = 1
pred_prob_thresh = 0.55
model_dirs = models/210501test/count_naive_bayes_210501test
