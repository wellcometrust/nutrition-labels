{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from nutrition_labels.grant_tagger import GrantTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed/training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(sample_not_relevant_range, num_repeats, vectorizer_type,model_type = 'naive_bayes'):\n",
    "    experiments_results = []\n",
    "    for n in sample_not_relevant_range:\n",
    "        average_results_train = None\n",
    "        average_results_test = None\n",
    "        for i in range(0, num_repeats):\n",
    "            # Take the average of 5 runs\n",
    "            grant_tagger = GrantTagger(\n",
    "                sample_not_relevant=n,\n",
    "                ngram_range=(1,2),\n",
    "                test_size=0.25,\n",
    "                irrelevant_sample_seed=i,\n",
    "                split_seed=i,\n",
    "                vectorizer_type = vectorizer_type,\n",
    "                model_type = model_type\n",
    "            )\n",
    "            X_train, X_test, y_train, y_test = grant_tagger.transform(data)\n",
    "            grant_tagger.fit(X_train, y_train)\n",
    "            results_train = grant_tagger.evaluate(X_train, y_train, print_results=False, average='weighted')\n",
    "            results_test = grant_tagger.evaluate(X_test, y_test, print_results=False, average='weighted')\n",
    "            if average_results_train: # will be the same for test too\n",
    "                for key in average_results_train.keys(): # will be the same for test too\n",
    "                    average_results_train[key] += results_train[key]\n",
    "                    average_results_test[key] += results_test[key]\n",
    "            else:\n",
    "                average_results_train = results_train\n",
    "                average_results_test = results_test\n",
    "        average_results_train = {key: value/num_repeats for key, value in average_results_train.items()}\n",
    "        average_results_test = {key: value/num_repeats for key, value in average_results_test.items()}\n",
    "        results_dict = {'sample_not_relevant': n}\n",
    "        for key, value in average_results_train.items():\n",
    "            results_dict[key+'_train'] = value\n",
    "        for key, value in average_results_test.items():\n",
    "            results_dict[key+'_test'] = value\n",
    "        experiments_results.append(results_dict)\n",
    "        \n",
    "    return experiments_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(experiments_results_df):\n",
    "    fig, (ax1,ax2,ax3,ax4) = plt.subplots(nrows=1, ncols=4, figsize=(15, 4))\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='accuracy_train',ax=ax1, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='accuracy_test', color='red', ax=ax1, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='f1_train',ax=ax2, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='f1_test', color='red', ax=ax2, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='precision_score_train',ax=ax3, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='precision_score_test', color='red', ax=ax3, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='recall_score_train',ax=ax4, marker='.')\n",
    "    experiments_results_df.plot(kind='line', x='sample_not_relevant', y='recall_score_test', color='red', ax=ax4, marker='.')\n",
    "    # fig.savefig(f'sample_not_relevant_{num_repeats}reps_{vectorizer_type}.png')\n",
    "    # plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_experiments_results = run_experiment(\n",
    "    range(40, 1000, 40), num_repeats=10, vectorizer_type='count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_results_df = pd.DataFrame(count_experiments_results)\n",
    "plot_results(experiments_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_experiments_results = run_experiment(\n",
    "    range(40, 600, 40), num_repeats=10, vectorizer_type='tfidf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_results_df = pd.DataFrame(tfidf_experiments_results)\n",
    "plot_results(experiments_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.loc[data['Relevance code'] == 1]))\n",
    "print(len(data.loc[data['Relevance code'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore all the results of those with good parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_setting(data, vectorizer_type, n):\n",
    "    i = 4\n",
    "    grant_tagger = GrantTagger(\n",
    "                sample_not_relevant=n,\n",
    "                ngram_range=(1,2),\n",
    "                test_size=0.25,\n",
    "                irrelevant_sample_seed=i,\n",
    "                split_seed=i,\n",
    "                vectorizer_type = vectorizer_type\n",
    "                )\n",
    "    X_train, X_test, y_train, y_test = grant_tagger.transform(data)\n",
    "    grant_tagger.fit(X_train, y_train)\n",
    "    results_train = grant_tagger.evaluate(X_train, y_train, average='weighted')\n",
    "    results_test = grant_tagger.evaluate(X_test, y_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_setting(data, 'tfidf', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_setting(data, 'tfidf', 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_setting(data, 'count', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_setting(data, 'count', 880)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing Bert Vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert and Naive Bayes model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "bert_bayes_experiments_results = run_experiment(\n",
    "    range(40, 600, 40), num_repeats=10, vectorizer_type='bert'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiments_results_df = pd.DataFrame(bert_bayes_experiments_results)\n",
    "plot_results(experiments_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert and SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "bert_svm_experiments_results = run_experiment(\n",
    "    range(40, 600, 40), num_repeats=10, vectorizer_type='bert', model_type='SVM'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiments_results_df = pd.DataFrame(bert_svm_experiments_results)\n",
    "plot_results(experiments_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert and Logistic regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "bert_logreg_experiments_results = run_experiment(\n",
    "    range(40, 600, 40), num_repeats=10, vectorizer_type='bert', model_type='log_reg'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiments_results_df = pd.DataFrame(bert_logreg_experiments_results)\n",
    "plot_results(experiments_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}